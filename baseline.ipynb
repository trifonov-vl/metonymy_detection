{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, name, tokens, entities):\n",
    "        self.name = name #book name\n",
    "        self.tokens = tokens\n",
    "        self.entities = entities\n",
    "    def __repr__(self):\n",
    "        return \"Tokens:{}\\nEntities:{}\\n\".format(self.tokens, self.entities)\n",
    "\n",
    "class Entity:\n",
    "    def __init__(self, id, ent_type, start_token, end_token, interval):\n",
    "        self.id = id #id in objects file\n",
    "        self.ent_type = ent_type\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.interval = interval\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \" id: \" + str(self.id) + \" type: \" + self.ent_type + \" start: \" + str(self.start_token) + \" end: \" + str(\n",
    "            self.end_token) + \"interval: \" + str(self.interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"testset\"\n",
    "dev_path = \"devset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dialent.task1.util import loadAllStandard\n",
    "\n",
    "def get_docs_from_std(path):\n",
    "    standard = loadAllStandard(path)\n",
    "    docs = []\n",
    "    for st in standard:\n",
    "        tokens = [token.text for token in st.tokens]\n",
    "        entities = []\n",
    "        for entity in st.makeTokenSets():\n",
    "            symb_start = entity.toInterval().start\n",
    "            symb_end = entity.toInterval().end\n",
    "            token_indexes = []\n",
    "            for idx, token in enumerate(st.tokens):\n",
    "                if symb_start <= token.start <= token.end <= symb_end:\n",
    "                    token_indexes.append(idx)\n",
    "            token_start = min(token_indexes)\n",
    "            token_end = max(token_indexes)+1\n",
    "            entities.append(Entity(entity.id, entity.tag, token_start, token_end, entity.toInterval()))\n",
    "        docs.append(Document(st.name, tokens, entities))\n",
    "    return docs\n",
    "\n",
    "def get_samples_from_std(docs, word_window_size, embedding_model, word_emb_size):   \n",
    "    samples_x = []\n",
    "    samples_y = []\n",
    "    for doc in docs:\n",
    "        for ent in doc.entities:\n",
    "            if ent.ent_type in {\"loc\", \"locorg\"}:\n",
    "                sample = [0] * word_emb_size * word_window_size * 2\n",
    "                entity_center = (ent.start_token+ent.end_token)//2\n",
    "                for i in range(1, word_window_size + 1):\n",
    "                    #left part of the window\n",
    "                    token_pos = entity_center - i\n",
    "                    if token_pos >= 0:\n",
    "                        word = doc.tokens[token_pos]\n",
    "                        if word in embedding_model:\n",
    "                            sample[(i - 1) * word_emb_size: i * word_emb_size] = embedding_model[word]\n",
    "\n",
    "                    #right part of the window\n",
    "                    token_pos = entity_center + i - 1\n",
    "                    if token_pos < len(doc.tokens):\n",
    "                        word = doc.tokens[token_pos]\n",
    "                        if word in embedding_model:\n",
    "                            sample[(word_window_size + i - 1) * word_emb_size:(word_window_size + i) * word_emb_size] = embedding_model[word]\n",
    "                        \n",
    "                samples_x.append(sample)\n",
    "                if ent.ent_type == \"loc\":\n",
    "                    label = 0\n",
    "                else:\n",
    "                    label = 1\n",
    "                samples_y.append(label)\n",
    "    return samples_x, samples_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def write_docs(docs, path, pred):\n",
    "    j = 0\n",
    "    \n",
    "    for doc in test_docs:\n",
    "        for i, ent in enumerate(doc.entities):\n",
    "            if ent.ent_type in {\"loc\", \"locorg\"}:\n",
    "                doc.entities[i].ent_type = \"loc\" if pred[j] == 0 else \"locorg\"\n",
    "                j += 1\n",
    "    \n",
    "    for doc in docs:\n",
    "        object_file_path = os.path.join(path, doc.name+\".task1\")\n",
    "        \n",
    "        lines_to_write = []\n",
    "            \n",
    "        for entity in doc.entities:\n",
    "            str_format = '{} {} {}\\n'.format(entity.ent_type, entity.interval.start,entity.interval.end-entity.interval.start+1)\n",
    "            lines_to_write.append(str_format)\n",
    "        \n",
    "        with open(object_file_path, \"w\") as f:\n",
    "            f.writelines(lines_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_window_size = 3\n",
    "word_emb_size = 300\n",
    "\n",
    "embedding_model = {}\n",
    "\n",
    "with open('word_vec.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        split = line.strip().split(' ')\n",
    "        embedding_model[split[0]] = [float(num) for num in split[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load the standard of book_3954:\n",
      "Unknown mention tag: Facility\n"
     ]
    }
   ],
   "source": [
    "dev_docs = get_docs_from_std(dev_path)\n",
    "test_docs = get_docs_from_std(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1076           53.28s\n",
      "         2           0.9276           55.36s\n",
      "         3           0.7958           55.47s\n",
      "         4           0.7060           55.86s\n",
      "         5           0.6405           55.70s\n",
      "         6           0.5910           55.40s\n",
      "         7           0.5480           55.30s\n",
      "         8           0.5058           55.12s\n",
      "         9           0.4760           54.80s\n",
      "        10           0.4430           54.47s\n",
      "        20           0.2570           52.09s\n",
      "        30           0.1716           48.60s\n",
      "        40           0.1199           45.51s\n",
      "        50           0.0905           42.91s\n",
      "        60           0.0646           40.58s\n",
      "        70           0.0493           38.19s\n",
      "        80           0.0390           35.83s\n",
      "        90           0.0296           33.63s\n",
      "       100           0.0230           31.53s\n",
      "       200           0.0071           10.49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.3, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=250, presort='auto', random_state=777,\n",
       "              subsample=1.0, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "x, y = get_samples_from_std(dev_docs, word_window_size, embedding_model, word_emb_size)\n",
    "clf = GradientBoostingClassifier(n_estimators=250, learning_rate=0.3, random_state=777, verbose=True)\n",
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples, _ = get_samples_from_std(test_docs, word_window_size, embedding_model, word_emb_size)\n",
    "pred=clf.predict(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load the standard of book_3954:\n",
      "Unknown mention tag: Facility\n",
      "Type    P        R        F1       TP1      TP2      In Std.  In Test.\n",
      "per        0.9993   0.9993   0.9993  1342.00  1342.00     1343     1343\n",
      "loc        0.8150   0.7810   0.7977   467.83   467.83      599      574\n",
      "org        0.9895   0.9895   0.9895  1557.55  1557.55     1574     1574\n",
      "locorg     0.7411   0.8734   0.8018   552.83   552.83      633      746\n",
      "overall    0.9251   0.9448   0.9348  3912.22  3912.22     4141     4229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "write_docs(test_docs, \"out\", pred)\n",
    "\n",
    "output = subprocess.check_output(\"python3 t1_eval.py -s testset -t out\".split()).decode(\"utf-8\")\n",
    "\n",
    "for line in output.split('\\n'):\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
