{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "test_path = \"../resources/testset\"\n",
    "dev_path = \"../resources/devset\"\n",
    "\n",
    "test_files = [join(test_path,f) for f in listdir(test_path) if isfile(join(test_path, f))]\n",
    "dev_files = [join(dev_path,f) for f in listdir(dev_path) if isfile(join(dev_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, id, tokens, entities, sentences):\n",
    "        self.id = id #book id\n",
    "        self.tokens = tokens\n",
    "        self.entities = entities\n",
    "        self.sentences = sentences\n",
    "    def __repr__(self):\n",
    "        return \"Tokens:{}\\nEntities:{}\\nSentences:{}\\n\".format(self.tokens, self.entities, self.sentences)\n",
    "\n",
    "class Entity:\n",
    "    def __init__(self, id, ent_type, start_token, end_token, symbol_start_idx, symbol_len):\n",
    "        self.id = id #id in objects file\n",
    "        self.ent_type = ent_type#location or locorg\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.symbol_start_idx = symbol_start_idx\n",
    "        self.symbol_len = symbol_len\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \" id: \" + str(self.id) + \" type: \" + self.ent_type + \" start: \" + str(self.start_token) + \" end: \" + str(\n",
    "            self.end_token) + \" symbol_start: {} symbol_len: {}\".format(self.symbol_start_idx, self.symbol_len)\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, start_token, end_token):\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "    def __repr__(self):\n",
    "        return \" start: \" + str(self.start_token) + \" end: \" + str(\n",
    "            self.end_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_doc(doc_id, tokens_file, span_file, objects_file):\n",
    "    tokens = []\n",
    "    token_ids = []\n",
    "    sentences = []\n",
    "    entities = []\n",
    "    spans = []\n",
    "    \n",
    "    current_start = 0\n",
    "    current_id = 0\n",
    "    for line in tokens_file.strip().split('\\n'):\n",
    "        if line == \"\":\n",
    "            sentences.append(Sentence(current_start,current_id))\n",
    "            current_start = current_id\n",
    "            continue\n",
    "        \n",
    "        lst = line.strip().split(' ')\n",
    "        token = lst[-1]\n",
    "        token_id = int(lst[0])\n",
    "        \n",
    "        tokens.append(token)\n",
    "        token_ids.append(token_id)\n",
    "        current_id += 1\n",
    "    \n",
    "    sentences.append(Sentence(current_start, len(tokens)))\n",
    "    \n",
    "    for line in span_file.strip().split('\\n'):\n",
    "        \n",
    "        span_lst = line.strip().split(' ')\n",
    "        span_token_id = int(span_lst[4])\n",
    "        entity_len = int(span_lst[5])\n",
    "        entity_type = span_lst[1]\n",
    "        entity_symbol_start_idx = int(span_lst[2])\n",
    "        entity_symbol_len = int(span_lst[3])\n",
    "        span_id = int(span_lst[0])\n",
    "        \n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            if token_id == span_token_id:\n",
    "                entity_start = i\n",
    "                break\n",
    "        \n",
    "        spans.append((span_id, entity_start, entity_len, entity_symbol_start_idx, entity_symbol_len))\n",
    "    \n",
    "    for line in objects_file.strip().split('\\n'):\n",
    "            object_lst = line.strip().split(' ')\n",
    "            object_id = int(object_lst[0])\n",
    "            object_type = object_lst[1]\n",
    "            \n",
    "            span_ids = []\n",
    "            for i in range(2, len(object_lst)):\n",
    "                sp_id = object_lst[i]\n",
    "                if sp_id == '#':\n",
    "                    break\n",
    "                span_ids.append(int(sp_id))\n",
    "                \n",
    "            if object_type == \"Location\":\n",
    "                object_type = 'loc'\n",
    "            elif object_type == \"LocOrg\":\n",
    "                object_type = 'locorg'\n",
    "            elif object_type == \"Person\":\n",
    "                object_type = \"per\"\n",
    "            elif object_type == \"Org\":\n",
    "                object_type = \"org\"\n",
    "            elif object_type == \"Project\":\n",
    "                object_type = \"org\"\n",
    "                \n",
    "                            \n",
    "            entity_spans = []\n",
    "            \n",
    "            for span_id in span_ids:\n",
    "                for span in spans:\n",
    "                    if span[0] == span_id:\n",
    "                        entity_spans.append(span)\n",
    "                        break\n",
    "              \n",
    "            max_token_idx = None\n",
    "            min_token_idx = None\n",
    "            max_symbol_idx = None\n",
    "            min_symbol_idx = None\n",
    "            \n",
    "#             start_sorted_spans = sorted(entity_spans, key = lambda x: x[1])\n",
    "#             end_sorted_spans = sorted(entity_spans, key = lambda x: x[2])\n",
    "            \n",
    "#             min_token_idx = start_sorted_spans[0][1]\n",
    "#             max_token_idx = end_sorted_spans[-1][2]\n",
    "#             max_symbol_idx = end_sorted_spans[-1][4]\n",
    "#             min_symbol_idx = start_sorted_spans[0][3]\n",
    "            \n",
    "            \n",
    "            for span in entity_spans:\n",
    "                token_start = span[1]\n",
    "                token_end = span[2] + token_start\n",
    "                symbol_start = span[3]\n",
    "                symbol_end = span[4] + symbol_start\n",
    "                \n",
    "                if min_symbol_idx is None or min_symbol_idx > symbol_start:\n",
    "                    min_symbol_idx = symbol_start\n",
    "                    \n",
    "                if max_symbol_idx is None or max_symbol_idx < symbol_end:\n",
    "                    max_symbol_idx = symbol_end\n",
    "                    \n",
    "                if min_token_idx is None or min_token_idx > token_start:\n",
    "                    min_token_idx = token_start\n",
    "                \n",
    "                if max_token_idx is None or max_token_idx < token_end:\n",
    "                    max_token_idx = token_end\n",
    "            \n",
    "            entities.append(Entity(object_id, object_type, min_token_idx, \n",
    "                                   max_token_idx, min_symbol_idx, max_symbol_idx-min_symbol_idx))\n",
    "    return Document(doc_id, tokens, entities, sentences)   \n",
    "\n",
    "def read_docs(files):\n",
    "    re_pattern = \"\\d+\"\n",
    "    p = re.compile(re_pattern)\n",
    "    \n",
    "    files_with_id = dict()\n",
    "    for f in files:\n",
    "        search = p.search(f)\n",
    "        if search is None:\n",
    "            continue\n",
    "        file_id = int(search.group())\n",
    "        lst = files_with_id.setdefault(file_id, list())\n",
    "        lst.append(f)\n",
    "    \n",
    "    docs = []\n",
    "    for id, lst in files_with_id.items():\n",
    "        tokens = []\n",
    "        entities = []\n",
    "        \n",
    "        span_file = None\n",
    "        tokens_file = None\n",
    "        objects_file = None\n",
    "        \n",
    "        for file in lst:\n",
    "            with open(file, \"r\") as open_file:\n",
    "                file_content = open_file.read()\n",
    "                                      \n",
    "            if file[-5:] == \"spans\":\n",
    "                span_file = file_content\n",
    "            elif file[-7:] == \"objects\":\n",
    "                objects_file = file_content\n",
    "            elif file[-6:] == \"tokens\":\n",
    "                tokens_file = file_content\n",
    "                \n",
    "        docs.append(generate_doc(id, tokens_file, span_file, objects_file))\n",
    "    \n",
    "    return docs             \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def write_docs(docs, path):\n",
    "    for doc in docs:\n",
    "        doc_id = doc.id\n",
    "        object_file_path = os.path.join(path, \"book_\"+str(doc_id)+\".task1\")\n",
    "        \n",
    "        lines_to_write = []\n",
    "            \n",
    "        for entity in doc.entities:\n",
    "            str_format = '{} {} {}\\n'.format(entity.ent_type, entity.symbol_start_idx,entity.symbol_len+2)\n",
    "            lines_to_write.append(str_format)\n",
    "        \n",
    "        with open(object_file_path, \"w\") as f:\n",
    "            f.writelines(lines_to_write)\n",
    "\n",
    "# docs = read_docs(test_files)\n",
    "\n",
    "# print(docs[0].id)\n",
    "\n",
    "# write_docs(docs, \"../resources/out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = {}\n",
    "\n",
    "with open('../resources/word_vec.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        split = line.strip().split(' ')\n",
    "        embedding_model[split[0]] = [float(num) for num in split[1:]]\n",
    "\n",
    "word_emb_size = len(embedding_model['цикл'])\n",
    "embedding_model[\"\\\"\"] = [0]*word_emb_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "def find_entity_sentence_idx(doc, entity):\n",
    "    for sent in doc.sentences:\n",
    "        if sent.start_token <= entity.start_token< sent.end_token:\n",
    "            return sent\n",
    "\n",
    "def get_samples(docs, word_window_size, max_len, pos_dict=None):   \n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    POS_tags = set()\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc.pos_tags = []\n",
    "        for token in doc.tokens:\n",
    "            pos_tag = morph.parse(token)[0].tag.POS\n",
    "            doc.pos_tags.append(pos_tag)\n",
    "            POS_tags.add(pos_tag)\n",
    "    \n",
    "    if pos_dict is None: \n",
    "        pos_dict = {}\n",
    "\n",
    "        for i, tag in enumerate(sorted(POS_tags, key=str)):\n",
    "            pos_dict[tag] = i\n",
    "\n",
    "        pos_dict[\"OOV\"] = len(pos_dict)\n",
    "         \n",
    "    samples_x = []\n",
    "    samples_y = []\n",
    "    for doc in docs:\n",
    "        for ent in doc.entities:\n",
    "            if ent.ent_type in {\"loc\", \"locorg\", \"org\"}:\n",
    "                sent = find_entity_sentence_idx(doc, ent)\n",
    "                sample = [0] * word_emb_size * word_window_size * 2\n",
    "                for i in range(1, word_window_size + 1):\n",
    "                    #left part of the window\n",
    "                    token_pos = ent.start_token - i\n",
    "                    if token_pos >= sent.start_token:\n",
    "                        sample[(i - 1) * word_emb_size: i * word_emb_size] = embedding_model[doc.tokens[token_pos].lower()]\n",
    "                        pos_tag_id = pos_dict[doc.pos_tags[token_pos]]\n",
    "                    else:\n",
    "                        pos_tag_id = pos_dict[\"OOV\"]\n",
    "                    \n",
    "                    pos_one_hot = [0]*len(pos_dict)\n",
    "                    pos_one_hot[pos_tag_id] = 1\n",
    "                    sample += pos_one_hot\n",
    "                    \n",
    "                    \n",
    "                    #right part of the window\n",
    "                    token_pos = ent.end_token + i - 1\n",
    "                    if token_pos < sent.end_token:\n",
    "                        sample[(word_window_size + i - 1) * word_emb_size:(word_window_size + i) * word_emb_size] = embedding_model[\n",
    "                            doc.tokens[token_pos].lower()]\n",
    "                        pos_tag_id = pos_dict[doc.pos_tags[token_pos]]\n",
    "                    else:\n",
    "                        pos_tag_id = pos_dict[\"OOV\"]\n",
    "                        \n",
    "                    pos_one_hot = [0]*len(pos_dict)\n",
    "                    pos_one_hot[pos_tag_id] = 1\n",
    "                    sample += pos_one_hot\n",
    "                \n",
    "                entity_len_one_hot = [0]*(max_len+1)\n",
    "                entity_len = ent.end_token-ent.start_token\n",
    "                \n",
    "                if entity_len > max_len:\n",
    "                    entity_len = max_len\n",
    "                \n",
    "                entity_len_one_hot[entity_len] = 1\n",
    "                \n",
    "                sample += entity_len_one_hot\n",
    "                \n",
    "                samples_x.append(sample)\n",
    "                if ent.ent_type == \"loc\":\n",
    "                    label = 0\n",
    "#                 elif ent.ent_type == \"locorg\":\n",
    "#                     label = 1\n",
    "                else:\n",
    "                    label = 1\n",
    "                samples_y.append(label)\n",
    "    return samples_x, samples_y, pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_docs = read_docs(dev_files)\n",
    "word_window_size = 7\n",
    "x, y, pos_dict = get_samples(dev_docs, word_window_size, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3201"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=50000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='sigmoid',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(C=500, gamma=\"auto\", verbose=True, kernel=\"sigmoid\")\n",
    "\n",
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0990            1.28m\n",
      "         2           1.0275            1.28m\n",
      "         3           0.9684            1.27m\n",
      "         4           0.9200            1.26m\n",
      "         5           0.8784            1.25m\n",
      "         6           0.8428            1.25m\n",
      "         7           0.8115            1.24m\n",
      "         8           0.7837            1.23m\n",
      "         9           0.7593            1.23m\n",
      "        10           0.7370            1.22m\n",
      "        20           0.5896            1.15m\n",
      "        30           0.4994            1.10m\n",
      "        40           0.4365            1.03m\n",
      "        50           0.3944           57.65s\n",
      "        60           0.3533           53.67s\n",
      "        70           0.3211           49.77s\n",
      "        80           0.2983           45.78s\n",
      "        90           0.2746           41.87s\n",
      "       100           0.2518           38.09s\n",
      "       200           0.1209            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, verbose=True)\n",
    "\n",
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.44334193\n",
      "Iteration 2, loss = 0.31656572\n",
      "Iteration 3, loss = 0.25876692\n",
      "Iteration 4, loss = 0.20030087\n",
      "Iteration 5, loss = 0.15602256\n",
      "Iteration 6, loss = 0.12505845\n",
      "Iteration 7, loss = 0.10121408\n",
      "Iteration 8, loss = 0.08282268\n",
      "Iteration 9, loss = 0.06538469\n",
      "Iteration 10, loss = 0.06154723\n",
      "Iteration 11, loss = 0.05980468\n",
      "Iteration 12, loss = 0.06444928\n",
      "Iteration 13, loss = 0.04110933\n",
      "Iteration 14, loss = 0.03027712\n",
      "Iteration 15, loss = 0.02417509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size=16, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(600, 300), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=15, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(max_iter=15, hidden_layer_sizes=(600,300), activation=\"tanh\", solver=\"adam\", batch_size=16, learning_rate_init=0.001, verbose=True)\n",
    "\n",
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = read_docs(test_files)\n",
    "test_samples, _, _= get_samples(test_docs, word_window_size, 10, pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=clf.predict(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for doc in test_docs:\n",
    "    for i, ent in enumerate(doc.entities):\n",
    "        if ent.ent_type in {\"loc\", \"locorg\"}:\n",
    "            doc.entities[i].ent_type = \"loc\" if pred[j] == 0 else \"locorg\"\n",
    "            j += 1\n",
    "        if ent.ent_type == \"org\":\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_docs(test_docs, \"../resources/out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
