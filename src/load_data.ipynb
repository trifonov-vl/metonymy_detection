{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "test_path = \"../resources/testset\"\n",
    "dev_path = \"../resources/devset\"\n",
    "\n",
    "test_files = [join(test_path,f) for f in listdir(test_path) if isfile(join(test_path, f))]\n",
    "dev_files = [join(dev_path,f) for f in listdir(dev_path) if isfile(join(dev_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, id, tokens, entities, sentences):\n",
    "        self.id = id #book id\n",
    "        self.tokens = tokens\n",
    "        self.entities = entities\n",
    "        self.sentences = sentences\n",
    "\n",
    "class Entity:\n",
    "    def __init__(self, id, ent_type, start_token, end_token):\n",
    "        self.id = id #id in objects file\n",
    "        self.ent_type = ent_type#location or locorg\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, start_token, end_token):\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'end_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-96d03d666c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-96d03d666c7b>\u001b[0m in \u001b[0;36mread_docs\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mtokens_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjects_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-96d03d666c7b>\u001b[0m in \u001b[0;36mgenerate_doc\u001b[0;34m(doc_id, tokens_file, span_file, objects_file)\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEntity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'end_token'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def generate_doc(doc_id, tokens_file, span_file, objects_file):\n",
    "    tokens = []\n",
    "    sentences = []\n",
    "    entities = []\n",
    "    spans = []\n",
    "    \n",
    "    current_start = 0\n",
    "    current_id = 0\n",
    "    for line in tokens_file.strip().split('\\n'):\n",
    "        if line == \"\":\n",
    "            sentences.append(Sentence(current_start,current_id))\n",
    "            current_start = current_id\n",
    "            continue\n",
    "        \n",
    "        token = line.strip().split(' ')[-1]\n",
    "        \n",
    "        tokens.append(token)\n",
    "        current_id += 1\n",
    "    \n",
    "    for line in span_file.strip().split('\\n'):\n",
    "        \n",
    "        span_lst = line.strip().split(' ')\n",
    "        entity_start = int(span_lst[4])\n",
    "        entity_len = int(span_lst[5])\n",
    "        entity_type = span_lst[1]\n",
    "        span_id = int(span_lst[0])\n",
    "        \n",
    "        if entity_type != \"loc_name\" and entity_type != \"loc_descr\":\n",
    "            continue\n",
    "        \n",
    "        spans.append((span_id, entity_start, entity_len))\n",
    "    \n",
    "    for line in objects_file.strip().split('\\n'):\n",
    "            object_lst = line.strip().split(' ')\n",
    "            object_id = int(object_lst[0])\n",
    "            object_type = object_lst[1]\n",
    "            \n",
    "            span_ids = []\n",
    "            for i in range(2, len(object_lst)):\n",
    "                sp_id = object_lst[i]\n",
    "                if sp_id == '#':\n",
    "                    break\n",
    "                span_ids.append(int(sp_id))\n",
    "                \n",
    "            \n",
    "            if object_type != \"Location\" and object_type != \"LocOrg\":\n",
    "                continue\n",
    "            \n",
    "            entity_spans = []\n",
    "            \n",
    "            for span_id in span_ids:\n",
    "                for span in spans:\n",
    "                    if span[0] == span_id:\n",
    "                        entity_spans.append(span)\n",
    "                        break\n",
    "            \n",
    "            start = None\n",
    "            max_len = 0\n",
    "            \n",
    "            for span_id, entity_start, entity_len in entity_spans:\n",
    "                if entity_len > max_len:\n",
    "                    max_len = entity_len\n",
    "                    start = entity_start\n",
    "            \n",
    "            entities.append(Entity(object_id, object_type, start, start+max_len))\n",
    "    return Document(doc_id, tokens, entities, sentences)   \n",
    "\n",
    "def read_docs(files):\n",
    "    re_pattern = \"\\d+\"\n",
    "    p = re.compile(re_pattern)\n",
    "    \n",
    "    files_with_id = dict()\n",
    "    for f in files:\n",
    "        file_id = int(p.search(f).group())\n",
    "        lst = files_with_id.setdefault(file_id, list())\n",
    "        lst.append(f)\n",
    "    \n",
    "    docs = []\n",
    "    for id, lst in files_with_id.items():\n",
    "        tokens = []\n",
    "        entities = []\n",
    "        \n",
    "        span_file = None\n",
    "        tokens_file = None\n",
    "        objects_file = None\n",
    "        \n",
    "        for file in lst:\n",
    "            with open(file, \"r\") as open_file:\n",
    "                file_content = open_file.read()\n",
    "                                      \n",
    "            if file[-5:] == \"spans\":\n",
    "                span_file = file_content\n",
    "            elif file[-7:] == \"objects\":\n",
    "                objects_file = file_content\n",
    "            elif file[-6:] == \"tokens\":\n",
    "                tokens_file = file_content\n",
    "                \n",
    "        docs.append(generate_doc(id, tokens_file, span_file, objects_file))\n",
    "    \n",
    "    return docs             \n",
    "                     \n",
    "  \n",
    "docs = read_docs(dev_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
